{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c615b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/26 01:27:12 WARN Utils: Your hostname, DESKTOP-3ADPNV0 resolves to a loopback address: 127.0.1.1; using 172.25.27.116 instead (on interface eth0)\n",
      "22/08/26 01:27:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/26 01:27:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/26 01:27:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/26 01:27:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Cell to create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 ASSIGNMENT 1 DUSTIN\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bea7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in the raw files\n",
    "\n",
    "yellow = spark.read.parquet('../../mast30034-project-1-dustintano10/data/raw/yellow_taxi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb09ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55274200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows before preprocessing\n",
    "\n",
    "yellow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac50f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column called day_of_week to get day of week for the trip and is_weekend which identify if record is a \n",
    "# weekend or not\n",
    "# drop day_of_week since we only want to know if its weekend or not\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "yellow = yellow.withColumn(\"day_of_week\", date_format(col(\"tpep_pickup_datetime\"),\"E\"))\n",
    "\n",
    "yellow = yellow.withColumn(\"is_weekend\", col(\"day_of_week\").isin([\"Sat\", \"Sun\"]).cast(\"boolean\"))\n",
    "\n",
    "yellow = yellow.drop(\"day_of_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a381f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates the date and time from both the pickup and dropoff date time columns\n",
    "# creates new columns for pickup/dropoff date\n",
    "\n",
    "yellow = yellow.withColumn(\"pickup_date\",\n",
    "                 to_date(col(\"tpep_pickup_datetime\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "yellow = yellow.withColumn(\"dropoff_date\",\n",
    "                 to_date(col(\"tpep_dropoff_datetime\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "# ensure passenger_count is not 0\n",
    "yellow = yellow.where( (F.col('passenger_count') > 0) )\n",
    "\n",
    "#drop columns that are not important for analysis\n",
    "yellow = yellow.drop(\"extra\", \"mta_tax\", \"congestion_surcharge\", \"airport_fee\", \"improvement_surcharge\",\n",
    "                    \"passenger_count\", \"store_and_fwd_flag\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368ad2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all other payment_types as tips are only counted with credit card payment\n",
    "yellow_credit = yellow.filter(F.col('payment_type') == 1)\n",
    "\n",
    "# remove records that has trips starting before the month of october\n",
    "yellow_credit = yellow_credit.filter(F.col('pickup_date') >= '2018-10-01')\n",
    "\n",
    "# remove other RatecodeID's as they make up such a small amount of the total dataset\n",
    "yellow_credit = yellow_credit.where( (F.col('RatecodeID') == 1) | (F.col('RatecodeID') == 2))\n",
    "\n",
    "# remove VendorID not being 1 or 2\n",
    "yellow_credit = yellow_credit.where( (F.col('VendorID') > 0 ) & (F.col('VendorID') < 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656a1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records which don't follow the initial amount of 2.5 from fare_amount\n",
    "\n",
    "yellow_credit = yellow_credit.where(F.col('fare_amount') >= 2.5)\n",
    "\n",
    "# remove records where trip distance is 0\n",
    "\n",
    "yellow_credit = yellow_credit.where(F.col('trip_distance') > 0)\n",
    "\n",
    "# remove records where the PU and DO location is not in the range\n",
    "\n",
    "yellow_credit = yellow_credit.where( ( F.col('PULocationID') < 264 ) & (F.col('DOLocationID') < 264) & \n",
    "                                    ( F.col('PULocationID') > 0) & (F.col('DOLocationID') > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16af47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a length of trip column in mins\n",
    "\n",
    "yellow_credit = yellow_credit.withColumn('trip_length', \n",
    "                         round((unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))/60, 4))\n",
    "\n",
    "# filters out trips that are negative and less than 2 minutes in time length\n",
    "yellow_credit = yellow_credit.where( (F.col('trip_length') > 2 ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b11a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed outliers for fare_amount\n",
    "# amounts for the upper and lower quantile is from the percentile_approx\n",
    "# for some reason it may change\n",
    "\n",
    "yellow_credit.select( percentile_approx(\"fare_amount\", [0.25, 0.75], 10000).alias(\"quantiles_fare\") )\n",
    "\n",
    "\n",
    "upper_q_fare = 15.0\n",
    "lower_q_fare = 6.5\n",
    "\n",
    "IQ_fare = upper_q_fare-lower_q_fare\n",
    "\n",
    "borderline_upper_fare = upper_q_fare + (1.5 * IQ_fare)\n",
    "borderline_lower_fare = lower_q_fare - (1.5 * IQ_fare)\n",
    "\n",
    "yellow_credit = yellow_credit.where( (F.col('fare_amount') <= borderline_upper_fare ) & \n",
    "                                       (F.col('fare_amount') >= borderline_lower_fare) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20acf743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33711796"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of records before being merged\n",
    "yellow_credit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f294a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the curated nba_attendance and convert the Date column into date type\n",
    "# Then we convert the whole pandas dataframe into a spark dataframe \n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "nba_attendance = pd.read_csv('../../mast30034-project-1-dustintano10/data/curated/nba_attendance_new.csv')\n",
    "\n",
    "nba_attendance['Date'] = pd.to_datetime(nba_attendance['Date'], format='%Y%m%d')\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"Date\", DateType(), True),\n",
    "StructField(\"Start(ET)\", StringType(), True),\n",
    "StructField(\"Attendance\", IntegerType(), True),\n",
    "StructField(\"Win\", StringType(), True),\n",
    "StructField(\"margin_victory/loss\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "nba_attendance_spark = spark.createDataFrame(nba_attendance, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c89a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins the nba attendance with the yellow_credit dataframe\n",
    "yellow_credit = yellow_credit.join(nba_attendance_spark, yellow_credit.pickup_date == nba_attendance_spark.Date, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf83760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters the data to only be records where the knicks are playing\n",
    "yellow_credit = yellow_credit.filter( F.col('Date').isNotNull() )\n",
    "\n",
    "# filter records where pickup_date and dropoff_date is not the same as we are focusing just on days the knicks are playing\n",
    "# they also only make a small percentage of the data\n",
    "\n",
    "yellow_credit = yellow_credit.where((F.col(\"pickup_date\") == F.col(\"dropoff_date\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9118f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the even more irrelevant columns\n",
    "yellow_credit = yellow_credit.drop(\"VendorID\", \"payment_type\", \"RatecodeID\", \"pickup_date\", \"dropoff_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5ab52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6405294"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows after preprocessing\n",
    "yellow_credit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a113ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the booleans into 0 and 1's\n",
    "# convert the location ID's into strings for one hot encoding\n",
    "\n",
    "yellow_credit = yellow_credit.withColumn('pickup_hour', hour(\"tpep_pickup_datetime\"))\n",
    "\n",
    "yellow_credit = yellow_credit.withColumn('dropoff_hour', hour(\"tpep_dropoff_datetime\"))\n",
    "\n",
    "yellow_credit = yellow_credit.withColumn('is_weekend_binary', F.when(yellow_credit.is_weekend == 'false', 0).otherwise(1))\n",
    "\n",
    "yellow_credit = yellow_credit.withColumn('Win_binary', F.when(yellow_credit.Win == 'No', 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37dcd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>trip_distance</th><th>PULocationID</th><th>DOLocationID</th><th>fare_amount</th><th>tip_amount</th><th>tolls_amount</th><th>total_amount</th><th>is_weekend</th><th>trip_length</th><th>Date</th><th>Start(ET)</th><th>Attendance</th><th>Win</th><th>margin_victory/loss</th><th>pickup_hour</th><th>dropoff_hour</th><th>is_weekend_binary</th><th>Win_binary</th><th>Start(ET)_NUMERIC</th></tr>\n",
       "<tr><td>2018-12-25 00:39:10</td><td>2018-12-25 01:04:33</td><td>2.56</td><td>162</td><td>263</td><td>16.5</td><td>4.58</td><td>0.0</td><td>22.88</td><td>false</td><td>25.3833</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 01:15:08</td><td>2018-12-25 08:45:04</td><td>7.85</td><td>140</td><td>213</td><td>26.0</td><td>5.56</td><td>0.0</td><td>33.36</td><td>false</td><td>449.9333</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>1</td><td>8</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 09:08:51</td><td>2018-12-25 09:13:27</td><td>0.9</td><td>163</td><td>162</td><td>5.0</td><td>1.45</td><td>0.0</td><td>7.25</td><td>false</td><td>4.6</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>9</td><td>9</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 10:29:03</td><td>2018-12-25 10:31:40</td><td>0.63</td><td>100</td><td>48</td><td>4.0</td><td>0.96</td><td>0.0</td><td>5.76</td><td>false</td><td>2.6167</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>10</td><td>10</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 10:58:19</td><td>2018-12-25 11:14:08</td><td>8.08</td><td>161</td><td>13</td><td>23.5</td><td>0.0</td><td>0.0</td><td>24.3</td><td>false</td><td>15.8167</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>10</td><td>11</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 15:52:23</td><td>2018-12-25 16:07:03</td><td>2.13</td><td>162</td><td>142</td><td>11.5</td><td>1.0</td><td>0.0</td><td>13.3</td><td>false</td><td>14.6667</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>15</td><td>16</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 00:31:15</td><td>2018-12-25 00:42:04</td><td>2.88</td><td>230</td><td>236</td><td>11.0</td><td>2.36</td><td>0.0</td><td>14.16</td><td>false</td><td>10.8167</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 00:51:53</td><td>2018-12-25 00:58:03</td><td>0.98</td><td>161</td><td>229</td><td>6.0</td><td>1.36</td><td>0.0</td><td>8.16</td><td>false</td><td>6.1667</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 03:53:56</td><td>2018-12-25 04:06:21</td><td>1.64</td><td>246</td><td>107</td><td>9.5</td><td>0.0</td><td>0.0</td><td>10.3</td><td>false</td><td>12.4167</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>3</td><td>4</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "<tr><td>2018-12-25 17:03:49</td><td>2018-12-25 17:07:48</td><td>0.83</td><td>237</td><td>141</td><td>5.0</td><td>2.0</td><td>0.0</td><td>7.8</td><td>false</td><td>3.9833</td><td>2018-12-25</td><td>12:00</td><td>19812</td><td>No</td><td>-14</td><td>17</td><td>17</td><td>0</td><td>0</td><td>1.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+-------------+------------+------------+-----------+----------+------------+------------+----------+-----------+----------+---------+----------+---+-------------------+-----------+------------+-----------------+----------+-----------------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|tolls_amount|total_amount|is_weekend|trip_length|      Date|Start(ET)|Attendance|Win|margin_victory/loss|pickup_hour|dropoff_hour|is_weekend_binary|Win_binary|Start(ET)_NUMERIC|\n",
       "+--------------------+---------------------+-------------+------------+------------+-----------+----------+------------+------------+----------+-----------+----------+---------+----------+---+-------------------+-----------+------------+-----------------+----------+-----------------+\n",
       "| 2018-12-25 00:39:10|  2018-12-25 01:04:33|         2.56|         162|         263|       16.5|      4.58|         0.0|       22.88|     false|    25.3833|2018-12-25|    12:00|     19812| No|                -14|          0|           1|                0|         0|              1.0|\n",
       "| 2018-12-25 01:15:08|  2018-12-25 08:45:04|         7.85|         140|         213|       26.0|      5.56|         0.0|       33.36|     false|   449.9333|2018-12-25|    12:00|     19812| No|                -14|          1|           8|                0|         0|              1.0|\n",
       "| 2018-12-25 09:08:51|  2018-12-25 09:13:27|          0.9|         163|         162|        5.0|      1.45|         0.0|        7.25|     false|        4.6|2018-12-25|    12:00|     19812| No|                -14|          9|           9|                0|         0|              1.0|\n",
       "| 2018-12-25 10:29:03|  2018-12-25 10:31:40|         0.63|         100|          48|        4.0|      0.96|         0.0|        5.76|     false|     2.6167|2018-12-25|    12:00|     19812| No|                -14|         10|          10|                0|         0|              1.0|\n",
       "| 2018-12-25 10:58:19|  2018-12-25 11:14:08|         8.08|         161|          13|       23.5|       0.0|         0.0|        24.3|     false|    15.8167|2018-12-25|    12:00|     19812| No|                -14|         10|          11|                0|         0|              1.0|\n",
       "| 2018-12-25 15:52:23|  2018-12-25 16:07:03|         2.13|         162|         142|       11.5|       1.0|         0.0|        13.3|     false|    14.6667|2018-12-25|    12:00|     19812| No|                -14|         15|          16|                0|         0|              1.0|\n",
       "| 2018-12-25 00:31:15|  2018-12-25 00:42:04|         2.88|         230|         236|       11.0|      2.36|         0.0|       14.16|     false|    10.8167|2018-12-25|    12:00|     19812| No|                -14|          0|           0|                0|         0|              1.0|\n",
       "| 2018-12-25 00:51:53|  2018-12-25 00:58:03|         0.98|         161|         229|        6.0|      1.36|         0.0|        8.16|     false|     6.1667|2018-12-25|    12:00|     19812| No|                -14|          0|           0|                0|         0|              1.0|\n",
       "| 2018-12-25 03:53:56|  2018-12-25 04:06:21|         1.64|         246|         107|        9.5|       0.0|         0.0|        10.3|     false|    12.4167|2018-12-25|    12:00|     19812| No|                -14|          3|           4|                0|         0|              1.0|\n",
       "| 2018-12-25 17:03:49|  2018-12-25 17:07:48|         0.83|         237|         141|        5.0|       2.0|         0.0|         7.8|     false|     3.9833|2018-12-25|    12:00|     19812| No|                -14|         17|          17|                0|         0|              1.0|\n",
       "+--------------------+---------------------+-------------+------------+------------+-----------+----------+------------+------------+----------+-----------+----------+---------+----------+---+-------------------+-----------+------------+-----------------+----------+-----------------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converted Start (ET) into the values 1-6\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "categorical = ['Start(ET)']\n",
    "\n",
    "indexers = []\n",
    "\n",
    "for column in categorical:\n",
    "    indexers.append(StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(yellow_credit))\n",
    "    \n",
    "pipeline = Pipeline(stages=indexers)\n",
    "yellow_indexed = pipeline.fit(yellow_credit).transform(yellow_credit)\n",
    "yellow_indexed.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "290fa244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saves the new yellow_credit dataframe\n",
    "yellow_indexed.write.mode('overwrite').parquet('../../mast30034-project-1-dustintano10/data/curated/yellow/yellow_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e306ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
